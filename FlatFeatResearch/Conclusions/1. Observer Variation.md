#### ICC(Intraclass Correlation Coefficient) 
- ICC is a statistical measure that quantifies **how consistent or reliable measurements are when made by different raters**.
- **Values range from 0 to 1**:
	- **< 0.5** → poor reliability
	- **0.5–0.75** → moderate reliability
	- **0.75–0.9** → good reliability
	- **> 0.9** → excellent reliability
- **CI95%:** The 95% confidence interval for the ICC, showing the range within which the true reliability of the measurements is expected to lie with 95% confidence.
- We will use this data to understand different types of ICC's:
	- | Subject | Rater A | Rater B |
		| --- | --- | --- |
		| 1 | 80 | 85 |
		| 2 | 70 | 75 |
		| 3 | 60 | 65 |
- ICC1 (One-way random effects, single rater):
	- Measures the **reliability of a single rater’s score** compared to others.
	- Focuses on whether **individual raters can consistently differentiate between subjects**, but does not account for systematic differences (bias) between raters.
	- Now, with our dataset, ICC1 looks at a single rater at a time, it does not care about the systematic differences between rater A and rater B. It checks how well a rater distinguishes subjects. In this case, (1>2>3), both the raters rank subjects perfectly, so ICC1 is very high. 
- ICC2 (Two-way random effects, single rater)
	- Measures the **reliability of a single rater’s score** assuming raters are randomly chosen from a larger population.
	- Focuses on whether **ratings are reliable and generalizable** to other raters, accounting for both subject differences and systematic differences (bias) between raters.
	- Now, with our dataset, ICC2 looks at a single rater but **also considers that raters may differ systematically(consistent bias)**.
		- Here, Rater B scores +5 higher than Rater A.
	    - ICC2 sees this difference and slightly reduces the reliability estimate compared to ICC1.
	    - Even so, because both raters rank subjects perfectly (1>2>3), ICC2 is still **high**, but a bit lower than ICC1.
- ICC3 (Two-way mixed effects, single rater)
	- Measures the **reliability of a single rater’s score** assuming the raters in your study are the **only raters of interest** (fixed raters).
	- Focuses on whether **these specific raters rank subjects consistently**, ignoring systematic differences (bias) between them.
	- Now, with our dataset, ICC3 looks at both Rater A and Rater B but **ignores that Rater B scores +5 higher than Rater A**.
		- It only checks whether the raters **consistently rank the subjects** (1>2>3).
		- Because both raters rank the subjects perfectly, ICC3 is **very high**, similar to ICC1, even though their absolute scores differ.
- ICC1k (One-way random effects, average of k raters)
	- Measures the **reliability of the average score** from all raters, instead of a single rater.
	- Focuses on whether **the average rating can consistently differentiate subjects**, reducing the impact of random error.
	- Now, with our dataset, the **average scores** for each subject are taken by taken by taken the average between rater A and B. 
		- Because averaging reduces random differences between raters, ICC1k is **even higher than ICC1**.
		- In this dataset, the average clearly ranks the subjects perfectly (1>2>3), so ICC1k is **very high (~0.97–0.99)**.
- ICC2k (Two-way random effects, average of k raters)
	- Measures the **reliability of the average score** from all raters, assuming raters are **randomly chosen from a larger population**.
	- Focuses on whether **the averaged ratings are generalizable** to other raters beyond the ones in your study.
	- Measures **generalizable reliability** — if you picked different raters from the population, the reliability may change.
	- ICC2k looks at these averages but **also accounts for systematic differences** between raters (here, Rater B is +5 higher than A).
		- Because averaging reduces random error, ICC2k is **very high**, but may be **slightly lower than ICC1k** if raters differ systematically.
		- In this dataset, the averaged scores rank the subjects perfectly (1>2>3), so ICC2k is **excellent (~0.97–0.99)**. 
		- If we had more raters with bigger systematic differences, ICC2k would usually be **slightly lower than ICC1k**, because it accounts for generalizability.
- ICC3k (Two-way mixed effects, average of k raters)
	- Measures the **reliability of the average score** from all raters, assuming the raters in your study are the **only raters of interest** (fixed raters).
	- Focuses on whether **the average rating from these specific raters consistently differentiates subjects**, ignoring systematic differences (bias) between raters.
	- Measures reliability **specific to these raters**, not generalizable to others.
	- ICC3k looks at these averages and **ignores that Rater B scores +5 higher than Rater A**.
		- Because the averaged scores rank the subjects perfectly (1>2>3), ICC3k is **very high (~0.97–0.99)**, similar to ICC1k.

The ICC results of our dataset is given below:
```
Type,Description,ICC,F,df1,df2,pval,CI95%
ICC1,Single raters absolute,0.8891183097390846,33.07448615355348,14,45,5.135360900730971e-19,[0.78 0.96]
ICC2,Single random raters,0.8890581853352099,32.44026594269689,14,42,5.3948151734134874e-18,[0.78 0.96]
ICC3,Single fixed raters,0.887134029793468,32.44026594269689,14,42,5.3948151734134874e-18,[0.77 0.96
ICC1k,Average raters absolute,0.9697652143299418,33.07448615355348,14,45,5.135360900730971e-19,[0.93 0.99]
ICC2k,Average random raters,0.9697473319116747,32.44026594269689,14,42,5.3948151734134874e-18,[0.93 0.99]
ICC3k,Average fixed raters,0.9691741121430256,32.44026594269689,14,42,5.3948151734134874e-18,[0.93 0.99]
```

### Bland-Altman Function
- Bland-Altman plots help you **see if there’s systematic bias** (one rater consistently scores higher or lower) and how much measurements **scatter around the mean**. Whereas, ICC gives a **numerical measure of reliability** (how consistent raters are), but it **doesn’t show the pattern of disagreement**.
- **What the plot will show:**
	1. **x-axis:** Mean of KP and KT measurements for each patient
	2. **y-axis:** Difference between KP and KT measurements
	3. **Red line:** Mean difference (bias)
	4. **Blue dashed lines:** ±1.96 × SD → limits of agreement
	- This will tell you **if KP and KT are consistently measuring the same thing**, and whether there’s **systematic bias** (e.g., KT always scores higher than KP).
- **Plot 1: bland_altman_cal_PA_T1_KP_vs_KT**
	- This plot compares two different measurement methods for some value, labeled "cal_PA_T1_KP" and "cal_PA_KT". Measurements of CAL pitch angle, done by Dr KP and Dr KT. 
	- The **red horizontal line** is very close to zero, meaning there is very little to no systematic bias. This means, on average, one measurement does not consistently give a higher or a lower value than the other. 
	- The **two blue dashed lines** represents the 95% limits of agreement. In this plot, the limits of agreement are roughly from -7 to +7. This tells you the range within which the differences between the two methods are likely to fall for any given measurement.
	- Most data points are clustered around the mean difference line, showing **good agreement** for most measurements. However, there's one notable **outlier** around x=14, with a difference greater than 15.  This point falls far outside the 95% limits of agreement, suggesting a **significant discrepancy** between the two methods for this particular measurement. This outlier could be a data entry error, a measurement error, or an actual physiological difference that warrants further investigation.
	- The plot does not show any obvious pattern where the difference increases or decreases as the magnitude of the measurement increases. The points appear randomly scattered around the mean difference line, suggesting that the **agreement between the two methods is consistent** across the range of measured values. 
- **Plot 2: bland_altman_cal_PA_T2_KP_vs_KT(different timeline)**
	- This plot compares two different measurement methods for some value, labeled "cal_PA_T1_KP" and "cal_PA_KT". Measurements of CAL pitch angle, done by Dr KP and Dr KT, at a different timeline.
	- The **red horizontal line** seems to suggest a **slight systematic bias**, where the "cal_PA_T2_KP" method tends to give a slightly lower value than the "cal_PA_KT" method, on average.
	-  The **two blue dashed lines** are approximately from -6.5 to +5.5. This tells you that for about 95% of the measurements, the difference between the two methods will fall within this range.
	- Most data points are clustered around the mean difference line, showing **good overall agreement**. Similar to the previous plot, there are two notable **outliers**, one around x=5.5 with a difference of approximately -9, and another around x=10 with a difference of approximately +9. These points fall outside the 95% limits of agreement, indicating **significant discrepancies** between the two methods for these specific measurements. These should be investigated further as potential data or measurement errors.
	- The plot does not show any obvious pattern where the difference increases or decreases as the magnitude of the measurement increases. The points appear randomly scattered around the mean difference line, suggesting that the **agreement between the two methods is relatively consistent** across the range of measured values
### Final Conclusion

- **Reliability (ICC):**
	- Single rater reliability is **good** (ICC1–ICC3 ≈ 0.89).
	- Average of two raters is **excellent** (ICC1k–ICC3k ≈ 0.97).
	- Raters rank subjects consistently; averaging improves reliability.
	- Systematic differences between raters (bias) are minor and do not affect ranking much.
- **Agreement (Bland-Altman):**
	- Most measurements between Dr KP and Dr KT are consistent.
	- Small mean differences indicate **minimal bias**.
	- Limits of agreement are reasonable (±5–7°).
	- Few outliers exist, suggesting occasional discrepancies that may need checking.
- **Overall:** 
	- Measurements are **highly reliable and mostly agree** between raters, with minor occasional differences.

## Problem Statement Points vs. What You Solved

1. Observer Variation
	- **Problem:** Clinicians’ measurements vary → intra- and inter-observer differences.
	- **Your Conclusion Solves:** ✅
		- ICC shows single rater reliability is good, average rater reliability is excellent.
		- Bland-Altman shows minimal bias and reasonable limits of agreement.
		- Overall: measurements are reliable and consistent between raters.
2. Dominant Parameter Identification
	- **Problem:** Clinicians want to know if all measurements are necessary or if a few are enough.
	- **Your Conclusion Solves:** ❌ (not yet)
		- ICC and Bland-Altman show reliability, **but do not identify which parameters independently predict AAFD**.
		- You know measurements are consistent, but not which are dominant for diagnosis.
3. **Diagnosis Automation**
    - **Problem:** Automate AAFD detection using radiographic measurements.
    - **Your Conclusion Solves:** ❌ (not yet)
        - ICC/Bland-Altman do not give thresholds or predictive models for Yes/No diagnosis.
        - This step requires combining measurements with clinical rules or statistical/machine learning models.
- **Simplified Diagnostic Process**
    - **Problem:** Reduce clinician workload by identifying fewer reliable measurements.
    - **Your Conclusion Solves:** Partially ✅
         - All measurements are reliable, so clinicians can trust the data.
		- Need to determine **which subset of these reliable measurements can accurately predict AAFD**, enabling a simplified, faster diagnostic process.