# üîç Feature Reliability Ranking (Bland‚ÄìAltman Analysis)

We evaluated each radiographic feature using Bland‚ÄìAltman statistics to quantify observer agreement. Features are ranked by their **Combined Score** (average across all four measurement contexts: Inter-observer T1, Inter-observer T2, Intra-observer KP, Intra-observer KT).

**Lower Combined Score = Better Reliability**

The Combined Score integrates:
- **Normalized SD**: Measures consistency (spread of differences)
- **Normalized MAD**: Measures absolute deviation magnitude
- **Normalized MeanDiff**: Measures systematic bias (though clinically less important)

---

## üèÜ Overall Feature Reliability Ranking

| Rank | Feature | Avg Combined Score | Best Context | Worst Context | Clinical Assessment |
|------|---------|-------------------|--------------|---------------|---------------------|
| **1** | **LL** | **0.0073** | Intra-KT (0.000) | Inter-T2 (0.027) | ‚úÖ **Most Reliable** - Exceptional consistency across all contexts |
| **2** | **ML** | **0.1564** | Inter-T1 (0.0003) | Inter-T2 (0.517) | ‚úÖ **Highly Reliable** - Strong reproducibility overall |
| **3** | **MCL** | **0.1710** | Inter-T1 (0.013) | Inter-T2 (0.505) | ‚úÖ **Reliable** - Good consistency across contexts |
| **4** | **MT_calA** | **0.5934** | Intra-KP (0.127) | Inter-T2 (1.161) | ‚ö° **Moderately Reliable** - Acceptable with careful measurement |
| **5** | **cal_PA** | **0.5467** | Inter-T1 (0.163) | Inter-T2 (1.304) | ‚ö° **Moderately Reliable** - Variable across contexts |
| **6** | **MA** | **0.8630** | Intra-KP (0.204) | Intra-KT (1.621) | ‚ö†Ô∏è **Poor Reliability** - High measurement variability |
| **7** | **TCA** | **0.9951** | Intra-KP (0.311) | Intra-KT (1.746) | ‚ö†Ô∏è **Poor Reliability** - Inconsistent measurements |
| **8** | **5MTCA** | **1.0014** | Inter-T1 (0.304) | Intra-KT (2.716) | ‚ö†Ô∏è **Poor Reliability** - Difficult to measure consistently |
| **9** | **TUA** | **1.1566** | Inter-T1 (0.277) | Inter-T2 (3.000) | ‚ùå **Very Poor Reliability** - Highly inconsistent |
| **10** | **1MTA** | **2.6698** | Inter-T2 (2.213) | Inter-T1 (3.000) | ‚ùå **Unacceptable Reliability** - Not suitable for clinical use |

---

## üìä Detailed Context Breakdown

### Inter-Observer Agreement (Time 1: KP vs KT)
| Rank | Feature | Combined Score | Interpretation |
|------|---------|----------------|----------------|
| 1 | ML | 0.0003 | ‚úÖ Exceptional inter-observer agreement |
| 2 | LL | 0.002 | ‚úÖ Excellent inter-observer agreement |
| 3 | MCL | 0.013 | ‚úÖ Very good agreement |
| 4 | MT_calA | 0.152 | ‚ö° Moderate agreement |
| 5 | cal_PA | 0.163 | ‚ö° Moderate agreement |
| 6 | MA | 0.253 | ‚ö†Ô∏è Poor agreement |
| 7 | TUA | 0.277 | ‚ö†Ô∏è Poor agreement |
| 8 | 5MTCA | 0.304 | ‚ö†Ô∏è Poor agreement |
| 9 | TCA | 0.418 | ‚ùå Very poor agreement |
| 10 | 1MTA | 3.000 | ‚ùå Unacceptable agreement |

### Inter-Observer Agreement (Time 2: KP vs KT)
| Rank | Feature | Combined Score | Interpretation |
|------|---------|----------------|----------------|
| 1 | LL | 0.027 | ‚úÖ Excellent inter-observer agreement |
| 2 | MCL | 0.505 | ‚ö° Moderate agreement |
| 3 | ML | 0.517 | ‚ö° Moderate agreement |
| 4 | MT_calA | 1.161 | ‚ö†Ô∏è Poor agreement |
| 5 | cal_PA | 1.304 | ‚ö†Ô∏è Poor agreement |
| 6 | MA | 1.375 | ‚ö†Ô∏è Poor agreement |
| 7 | TCA | 1.540 | ‚ö†Ô∏è Poor agreement |
| 8 | 5MTCA | 1.944 | ‚ùå Very poor agreement |
| 9 | 1MTA | 2.213 | ‚ùå Very poor agreement |
| 10 | TUA | 3.000 | ‚ùå Unacceptable agreement |

### Intra-Observer Agreement (KP: Time 1 vs Time 2)
| Rank | Feature | Combined Score | Interpretation |
|------|---------|----------------|----------------|
| 1 | LL | 0.000 | ‚úÖ Perfect repeatability |
| 2 | ML | 0.062 | ‚úÖ Excellent repeatability |
| 3 | MCL | 0.062 | ‚úÖ Excellent repeatability |
| 4 | MT_calA | 0.127 | ‚ö° Good repeatability |
| 5 | cal_PA | 0.188 | ‚ö° Moderate repeatability |
| 6 | MA | 0.204 | ‚ö° Moderate repeatability |
| 7 | TCA | 0.311 | ‚ö†Ô∏è Fair repeatability |
| 8 | 5MTCA | 0.394 | ‚ö†Ô∏è Fair repeatability |
| 9 | TUA | 0.621 | ‚ö†Ô∏è Poor repeatability |
| 10 | 1MTA | 3.000 | ‚ùå Unacceptable repeatability |

### Intra-Observer Agreement (KT: Time 1 vs Time 2)
| Rank | Feature | Combined Score | Interpretation |
|------|---------|----------------|----------------|
| 1 | LL | 0.000 | ‚úÖ Perfect repeatability |
| 2 | ML | 0.038 | ‚úÖ Exceptional repeatability |
| 3 | MCL | 0.104 | ‚úÖ Excellent repeatability |
| 4 | cal_PA | 0.532 | ‚ö° Moderate repeatability |
| 5 | MT_calA | 0.935 | ‚ö° Moderate repeatability |
| 6 | MA | 1.621 | ‚ö†Ô∏è Poor repeatability |
| 7 | TCA | 1.746 | ‚ö†Ô∏è Poor repeatability |
| 8 | TUA | 1.959 | ‚ö†Ô∏è Poor repeatability |
| 9 | 1MTA | 2.503 | ‚ùå Very poor repeatability |
| 10 | 5MTCA | 2.716 | ‚ùå Very poor repeatability |

---

## üéØ Key Clinical Insights

### Tier 1: Clinical Gold Standard (Avg Score < 0.20)
- **LL, ML, MCL**: These features demonstrate exceptional reliability across all measurement contexts
- Recommended as **primary diagnostic features** for AAFD assessment
- Can be measured consistently by different observers and across time

### Tier 2: Supplementary Features (Avg Score 0.20-0.70)
- **MT_calA, cal_PA**: Moderate reliability, acceptable for clinical use with proper training
- Use as **secondary diagnostic indicators**
- Consider averaging multiple measurements to improve reliability

### Tier 3: Research Only (Avg Score 0.70-1.50)
- **MA, TCA, 5MTCA, TUA**: Poor to very poor reliability
- **Not recommended** for routine clinical decision-making
- May be useful in research settings with highly trained observers

### Tier 4: Not Clinically Useful (Avg Score > 1.50)
- **1MTA**: Unacceptable reliability across all contexts
- Should be **excluded** from diagnostic protocols
- High variability makes it unsuitable for clinical or research use

---

## üìå Summary

**Most Reliable Features**: LL (0.0073), ML (0.1564), MCL (0.1710)
- Consistently low combined scores across all measurement contexts
- Excellent inter-observer and intra-observer agreement
- Recommended for clinical AAFD assessment

**Least Reliable Features**: TUA (1.1566), 1MTA (2.6698)
- High combined scores indicate poor measurement consistency
- Not suitable for clinical decision-making
- Should be avoided in diagnostic protocols

**Observer Performance**: KT demonstrates superior repeatability compared to KP, particularly evident in intra-observer measurements where KT achieves near-perfect scores for LL, ML, and MCL.